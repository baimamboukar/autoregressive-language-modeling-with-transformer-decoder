Name: "variant_2_wider_shallow_15m"

###### Tokenization --------------------------------
tokenization:
  token_type: "5k"
  token_map:
      'char': 'hw4lib/data/tokenizer_jsons/tokenizer_char.json'
      '1k'  : 'hw4lib/data/tokenizer_jsons/tokenizer_1000.json'
      '5k'  : 'hw4lib/data/tokenizer_jsons/tokenizer_5000.json'
      '10k' : 'hw4lib/data/tokenizer_jsons/tokenizer_10000.json'

###### Dataset Configuration -----------------------------------------------------
data:
  root                 : "/content/hw4_data/hw4p2_data"
  train_partition      : "train-clean-100"
  val_partition        : "dev-clean"
  test_partition       : "test-clean"
  subset               : 1.0
  batch_size           : 32
  NUM_WORKERS          : 4
  norm                 : 'global_mvn'
  num_feats            : 80

  # Strong SpecAugment for better generalization
  specaug                   : True
  specaug_conf:
    apply_freq_mask         : True
    freq_mask_width_range   : 15
    num_freq_mask           : 3
    apply_time_mask         : True
    time_mask_width_range   : 70
    num_time_mask           : 3

###### Model: Wider, Shallower 15M parameters ------------
model:
  input_dim: 80
  time_reduction: 8
  reduction_method: 'both'

  # Wider but shallower
  d_model: 320
  num_encoder_layers: 3
  num_decoder_layers: 3
  num_encoder_heads: 10
  num_decoder_heads: 10
  d_ff_encoder: 1280
  d_ff_decoder: 1280
  skip_encoder_pe: False
  skip_decoder_pe: False

  dropout: 0.12
  layer_drop_rate: 0.08
  weight_tying: True

###### Training Configuration ---------------------------------------------------
training:
  use_wandb                   : True
  wandb_run_id                : "none"
  resume                      : True
  gradient_accumulation_steps : 1
  wandb_project               : "HW4P2"

###### Loss Configuration -------------------------------------------------------
loss:
  label_smoothing: 0.12
  ctc_weight: 0.25

###### Optimizer Configuration --------------------------------------------------
optimizer:
  name: "adamw"
  lr: 0.0025
  weight_decay: 0.015
  adamw:
    betas: [0.9, 0.98]
    eps: 1.0e-6
    amsgrad: False

###### Scheduler ---------------------------------------
scheduler:
  name: "cosine"
  cosine:
    T_max: 18
    eta_min: 0.00001
    last_epoch: -1
  warmup:
    enabled: True
    type: "exponential"
    epochs: 2
    start_factor: 0.1
    end_factor: 1.0

###### Beam Search Configuration --------------------
beam_search:
  configs:
    beam_5:
      beam_width: 5
      temperature: 1.0
      repeat_penalty: 1.1
      length_penalty: 0.8
    beam_10:
      beam_width: 10
      temperature: 0.9
      repeat_penalty: 1.15
      length_penalty: 0.9
    beam_15:
      beam_width: 15
      temperature: 0.85
      repeat_penalty: 1.2
      length_penalty: 1.0