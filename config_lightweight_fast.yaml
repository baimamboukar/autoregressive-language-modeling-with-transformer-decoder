Name: "lightweight_fast_cer6"

###### Tokenization (5k for optimal performance) --------------------------------
tokenization:
  token_type: "5k"
  token_map:
      'char': 'hw4lib/data/tokenizer_jsons/tokenizer_char.json'
      '1k'  : 'hw4lib/data/tokenizer_jsons/tokenizer_1000.json'
      '5k'  : 'hw4lib/data/tokenizer_jsons/tokenizer_5000.json'
      '10k' : 'hw4lib/data/tokenizer_jsons/tokenizer_10000.json'

###### Dataset Configuration -----------------------------------------------------
data:
  root                 : "/content/hw4_data/hw4p2_data"
  train_partition      : "train-clean-100"
  val_partition        : "dev-clean"
  test_partition       : "test-clean"
  subset               : 1.0                # Use full dataset
  batch_size           : 32                 # Larger batch for efficiency
  NUM_WORKERS          : 4
  norm                 : 'global_mvn'
  num_feats            : 80

  ###### Moderate SpecAugment for speed/performance balance
  specaug                   : True
  specaug_conf:
    apply_freq_mask         : True
    freq_mask_width_range   : 10            # Moderate frequency masking
    num_freq_mask           : 2             # Fewer masks for speed
    apply_time_mask         : True
    time_mask_width_range   : 50            # Moderate time masking
    num_time_mask           : 2             # Fewer masks for speed

###### Lightweight High-Performance Architecture (12-15M parameters) ------------
model:
  # Speech embedding parameters
  input_dim: 80
  time_reduction: 8                         # Aggressive downsampling for speed
  reduction_method: 'both'                  # Both LSTM + Conv

  # Lightweight architecture optimized for speed and performance
  d_model: 256                              # Smaller but efficient
  num_encoder_layers: 4                     # Shallow encoder for speed
  num_decoder_layers: 4                     # Shallow decoder for speed
  num_encoder_heads: 8                      # Sufficient attention heads
  num_decoder_heads: 8                      # Sufficient attention heads
  d_ff_encoder: 1024                        # 4x model dimension (standard)
  d_ff_decoder: 1024                        # 4x model dimension (standard)
  skip_encoder_pe: False
  skip_decoder_pe: False

  # Light regularization for speed
  dropout: 0.1
  layer_drop_rate: 0.05
  weight_tying: True                        # Reduces parameters significantly

###### Training Configuration ---------------------------------------------------
training:
  use_wandb                   : True
  wandb_run_id                : "none"
  resume                      : True
  gradient_accumulation_steps : 1           # No accumulation for speed
  wandb_project               : "HW4P2"
  max_grad_norm              : 1.0          # Gradient clipping for stability

###### Loss Configuration -------------------------------------------------------
loss:
  label_smoothing: 0.1                      # Moderate label smoothing
  ctc_weight: 0.2                           # Lower CTC weight for stability

###### Optimizer Configuration --------------------------------------------------
optimizer:
  name: "adamw"
  lr: 0.0008                                # Stable learning rate
  weight_decay: 0.01                        # Moderate weight decay

  # AdamW specific parameters
  adamw:
    betas: [0.9, 0.98]
    eps: 1.0e-6
    amsgrad: False

###### Simple Scheduler for Fast Training ---------------------------------------
scheduler:
  name: "cosine"

  # Simple cosine annealing for fast convergence
  cosine:
    T_max: 20                               # Short cycle for fast training
    eta_min: 0.00001
    last_epoch: -1

  # Short warmup for stability
  warmup:
    enabled: True
    type: "exponential"
    epochs: 2                               # Very short warmup
    start_factor: 0.1
    end_factor: 1.0

###### Beam Search Configuration (for final evaluation only) --------------------
beam_search:
  configs:
    beam_5:
      beam_width: 5
      temperature: 1.0
      repeat_penalty: 1.1
      length_penalty: 0.8

    beam_10:
      beam_width: 10
      temperature: 0.9
      repeat_penalty: 1.15
      length_penalty: 0.9

    beam_15:
      beam_width: 15
      temperature: 0.85
      repeat_penalty: 1.2
      length_penalty: 1.0