Name: "variant_3_deeper_narrow_15m"

###### Tokenization --------------------------------
tokenization:
  token_type: "5k"
  token_map:
      'char': 'lib/data/tokenizer_jsons/tokenizer_char.json'
      '1k'  : 'lib/data/tokenizer_jsons/tokenizer_1000.json'
      '5k'  : 'lib/data/tokenizer_jsons/tokenizer_5000.json'
      '10k' : 'lib/data/tokenizer_jsons/tokenizer_10000.json'

###### Dataset Configuration -----------------------------------------------------
data:
  root                 : "./data_subset/P2.data"
  train_partition      : "train-clean-100"
  val_partition        : "dev-clean"
  test_partition       : "test-clean"
  subset               : 1.0
  batch_size           : 28
  NUM_WORKERS          : 4
  norm                 : 'global_mvn'
  num_feats            : 80

  # Light SpecAugment for deeper model
  specaug                   : True
  specaug_conf:
    apply_freq_mask         : True
    freq_mask_width_range   : 8
    num_freq_mask           : 2
    apply_time_mask         : True
    time_mask_width_range   : 45
    num_time_mask           : 2

###### Model: Deeper, Narrower 15M parameters ------------
model:
  input_dim: 80
  time_reduction: 8
  reduction_method: 'both'

  # Deeper but narrower
  d_model: 224
  num_encoder_layers: 5
  num_decoder_layers: 5
  num_encoder_heads: 8
  num_decoder_heads: 8
  d_ff_encoder: 896
  d_ff_decoder: 896
  skip_encoder_pe: False
  skip_decoder_pe: False

  dropout: 0.08
  layer_drop_rate: 0.03
  weight_tying: True

###### Training Configuration ---------------------------------------------------
training:
  use_wandb                   : True
  wandb_run_id                : "none"
  resume                      : True
  gradient_accumulation_steps : 1
  wandb_project               : "P2"

###### Loss Configuration -------------------------------------------------------
loss:
  label_smoothing: 0.1
  ctc_weight: 0.35

###### Optimizer Configuration --------------------------------------------------
optimizer:
  name: "adamw"
  lr: 0.002
  weight_decay: 0.008
  adamw:
    betas: [0.9, 0.98]
    eps: 1.0e-6
    amsgrad: False

###### Scheduler ---------------------------------------
scheduler:
  name: "cosine"
  cosine:
    T_max: 20
    eta_min: 0.00001
    last_epoch: -1
  warmup:
    enabled: True
    type: "exponential"
    epochs: 4
    start_factor: 0.1
    end_factor: 1.0

###### Beam Search Configuration --------------------
beam_search:
  configs:
    beam_5:
      beam_width: 5
      temperature: 1.0
      repeat_penalty: 1.1
      length_penalty: 0.8
    beam_10:
      beam_width: 10
      temperature: 0.9
      repeat_penalty: 1.15
      length_penalty: 0.9
    beam_15:
      beam_width: 15
      temperature: 0.85
      repeat_penalty: 1.2
      length_penalty: 1.0