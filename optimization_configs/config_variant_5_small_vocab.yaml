Name: "variant_5_small_vocab_15m"

###### Tokenization - Smaller vocabulary for diversity --------------------------------
tokenization:
  token_type: "1k"  # Smaller vocabulary
  token_map:
      'char': 'lib/data/tokenizer_jsons/tokenizer_char.json'
      '1k'  : 'lib/data/tokenizer_jsons/tokenizer_1000.json'
      '5k'  : 'lib/data/tokenizer_jsons/tokenizer_5000.json'
      '10k' : 'lib/data/tokenizer_jsons/tokenizer_10000.json'

###### Dataset Configuration -----------------------------------------------------
data:
  root                 : "./data_subset/P2.data"
  train_partition      : "train-clean-100"
  val_partition        : "dev-clean"
  test_partition       : "test-clean"
  subset               : 1.0
  batch_size           : 36
  NUM_WORKERS          : 4
  norm                 : 'global_mvn'
  num_feats            : 80

  # Aggressive SpecAugment for small vocab
  specaug                   : True
  specaug_conf:
    apply_freq_mask         : True
    freq_mask_width_range   : 18
    num_freq_mask           : 3
    apply_time_mask         : True
    time_mask_width_range   : 80
    num_time_mask           : 3

###### Model: Optimized for smaller vocabulary 15M parameters ------------
model:
  input_dim: 80
  time_reduction: 6
  reduction_method: 'lstm'  # Different reduction method

  # Larger model compensating for smaller vocab
  d_model: 288
  num_encoder_layers: 4
  num_decoder_layers: 4
  num_encoder_heads: 12
  num_decoder_heads: 12
  d_ff_encoder: 1152
  d_ff_decoder: 1152
  skip_encoder_pe: False
  skip_decoder_pe: False

  dropout: 0.15
  layer_drop_rate: 0.1
  weight_tying: True

###### Training Configuration ---------------------------------------------------
training:
  use_wandb                   : True
  wandb_run_id                : "none"
  resume                      : True
  gradient_accumulation_steps : 1
  wandb_project               : "P2"

###### Loss Configuration -------------------------------------------------------
loss:
  label_smoothing: 0.18
  ctc_weight: 0.2

###### Optimizer Configuration --------------------------------------------------
optimizer:
  name: "adamw"
  lr: 0.0035
  weight_decay: 0.02
  adamw:
    betas: [0.9, 0.98]
    eps: 1.0e-6
    amsgrad: False

###### Scheduler ---------------------------------------
scheduler:
  name: "cosine"
  cosine:
    T_max: 12
    eta_min: 0.00001
    last_epoch: -1
  warmup:
    enabled: True
    type: "exponential"
    epochs: 2
    start_factor: 0.1
    end_factor: 1.0

###### Beam Search Configuration --------------------
beam_search:
  configs:
    beam_5:
      beam_width: 5
      temperature: 1.0
      repeat_penalty: 1.1
      length_penalty: 0.8
    beam_10:
      beam_width: 10
      temperature: 0.9
      repeat_penalty: 1.15
      length_penalty: 0.9
    beam_15:
      beam_width: 15
      temperature: 0.85
      repeat_penalty: 1.2
      length_penalty: 1.0