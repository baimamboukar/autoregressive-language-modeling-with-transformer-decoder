# -*- coding: utf-8 -*-
"""HW4P2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rD8I7J-w6vvYCbRkY2ljhr5N-AUqUsBb

# HW4P2: Automatic Speech Recognition with an Encoder-Decoder Transformer

# Schedule:
- Checkpoint Submission (DUE 21 November 2025 @ 11:59PM EST)
- Kaggle Submission (DUE 5 December 2025 @ 11:59PM EST | Slack Deadline is 11 December 2025 @ 11:59PM EST)
- Code Submission (DUE 7 December 2025 @ 11:59PM EST OR Day-of Slack submission)

## Requirement Acknowledgement
Setting the below flag to True indicates full understanding and acceptance of the following:
1. Slack days may ONLY be used on P2 FINAL (not checkpoint) submission. I.e. you may use slack days to submit final P2 kaggle scores (such as this one) later on the **SLACK KAGGLE COMPETITION** at the expense of your Slack days.
2. The final autolab **code submission is due 48 hours after** the conclusion of the Kaggle Deadline (or, the same day as your final kaggle submission).
3. We will require your kaggle username here, and then we will pull your official PRIVATE kaggle leaderboard score. This submission may result in slight variance in scores/code, but we will check for acceptable discrepancies. Any discrepancies related to modifying the submission code (at the bottom of the notebook) will result in an AIV.
4. You are NOT allowed to use any code that will pre-load models (such as those from Hugging Face, etc.).
   You MAY use models described by papers or articles, but you MUST implement them yourself through fundamental PyTorch operations (i.e. Linear, Conv2d, etc.).
5. You are NOT allowed to use any external data/datasets at ANY point of this assignment.
6. You may work with teammates to run ablations/experiments, BUT you must submit your OWN code and your OWN results.
7. Failure to comply with the prior rules will be considered an Academic Integrity Violation (AIV).
8. Late submissions MUST be submitted through the Slack Kaggle (see writeup for details). Any submissions made to the regular Kaggle after the original deadline will NOT be considered, no matter how many slack days remain for the student.
"""

ACKNOWLEDGED = True #TODO: Only set Acknowledged to True if you have read the above acknowlegements and agree to ALL of them.

"""# Setup
-  Follow the setup instructions based on your preferred environment!

## Local

One of our key goals in designing this assignment is to allow you to complete most of the preliminary implementation work locally.  
We highly recommend that you **pass all tests locally** using the provided `hw4_data_subset` before moving to a GPU runtime.  
To do this, simply:

### Create a new conda environment
```bash
# Be sure to deactivate any active environments first
conda create -n hw4 python=3.12.4
```

### Activate the conda environment
```bash
conda activate hw4
```

### Install the dependencies using the provided `requirements.txt`
```bash
pip install --no-cache-dir --ignore-installed -r requirements.txt
```

### Ensure that your notebook is in the same working directory as the `Handout`
This can be achieved by:
1. Physically moving the notebook into the handout directory.
2. Changing the notebook‚Äôs current working directory to the handout directory using the os.chdir() function.

### Open the notebook and select the newly created environment from the kernel selector.

If everything was done correctly, You should see atleast the following files in your current working directory after running `!ls`:
```
.
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ hw4lib/
‚îú‚îÄ‚îÄ mytorch/
‚îú‚îÄ‚îÄ tests/
‚îî‚îÄ‚îÄ hw4_data_subset/
```

## Colab

### Step 1: Get your handout
- See writeup for recommended approaches.
"""

# Example: My preferred approach
import os
# Settings -> Developer Settings -> Personal Access Tokens -> Token (classic)
os.environ['GITHUB_TOKEN'] = "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"

GITHUB_USERNAME = "baimamboukar"
REPO_NAME       = "IDL-HW4"
UPSTREAM_REPO = "IDLF25-HW4"
TOKEN = os.environ.get("GITHUB_TOKEN")
repo_url        = f"https://{TOKEN}@github.com/{GITHUB_USERNAME}/IDLF25-HW4 IDL-HW4"
!git clone {repo_url}

# To pull latest changes (Must be in the repo dir, use pwd/ls to verify)
!cd {REPO_NAME} && git pull

"""### Step 2: Install Dependencies
- `NOTE`: Your runtime will be restarted to ensure all dependencies are updated.
- `NOTE`: You will see a runtime crashed message, this was intentionally done. Simply move on to the next cell.
"""

# Commented out IPython magic to ensure Python compatibility.
!pwd
# %cd /content/

# Commented out IPython magic to ensure Python compatibility.
# %pip install --no-deps -r IDL-HW4/requirements.txt --quiet
import os
os.kill(os.getpid(), 9) # NOTE: This will restart the your colab Python runtime (required)!

!pip install transformers -U --quiet

"""### Step 3: Obtain Data

- `NOTE`: This process will automatically download and unzip data for both `HW4P1` and `HW4P2`.  

"""

!mkdir /root/.kaggle
with open("/root/.kaggle/kaggle.json", "w+") as f:

    f.write('{"username":"bbaimamb","key":"8fcf5086644f33fb2aa3f4a83d38ceb6"}')

!chmod 600 /root/.kaggle/kaggle.json

!curl -L -o /content/f25-hw4-data.zip https://www.kaggle.com/api/v1/datasets/download/cmu11785/f25-11785-hw4-data
!unzip -q -o /content/f25-hw4-data.zip -d /content/hw4_data
!rm -rf /content/f25-hw4-data.zip
!du -h --max-depth=2 /content/hw4_data

"""### Step 4: Move to Handout Directory
You must be within the handout directory for the library imports to work!

- `NOTE`: You may have to repeat running this command anytime you restart your runtime.
- `NOTE`: You can do a `pwd` to check if you are in the right directory.
- `NOTE`: The way it is setup currently, Your data directory should be one level up from your project directory. Keep this in mind when you are setting your `root` in the config file.

If everything was done correctly, You should see atleast the following files in your current working directory after running `!ls`:
```
.
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ hw4lib/
‚îú‚îÄ‚îÄ mytorch/
‚îú‚îÄ‚îÄ tests/
‚îî‚îÄ‚îÄ hw4_data_subset/

```
"""

import os
os.chdir('IDL-HW4')
!ls

"""# Imports
- If your setup was done correctly, you should be able to run the following cell without any issues.
"""

from hw4lib.data import (
    H4Tokenizer,
    ASRDataset,
    verify_dataloader
)
from hw4lib.model import (
    DecoderOnlyTransformer,
    EncoderDecoderTransformer
)
from hw4lib.utils import (
    create_scheduler,
    create_optimizer,
    plot_lr_schedule
)
from hw4lib.trainers import (
    ASRTrainer,
    ProgressiveTrainer
)
from torch.utils.data import DataLoader
import yaml
import gc
import torch
from torchinfo import summary
import os
import json
import wandb
import pandas as pd
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

"""# Implementations
- `NOTE`: All of these implementations have detailed specification, implementation details, and hints in their respective source files. Make sure to read all of them in their entirety to understand the implementation details!

## Dataset Implementation
- Implement the `ASRDataset` class in `hw4lib/data/asr_dataset.py`.
- You will have to implement parts of `__init__` and completely implement the `__len__`, `__getitem__` and `collate_fn` methods.
- Run the cell below to check your implementation.
"""

!python -m tests.test_dataset_asr

"""## Model Implementations

Overview:

- Implement the `CrossAttentionLayer` class in `hw4lib/model/sublayers.py`.
- Implement the `CrossAttentionDecoderLayer` class in `hw4lib/model/decoder_layers.py`.
- Implement the `SelfAttentionEncoderLayer` class in `hw4lib/model/encoder_layers.py`. This will be mostly a copy-paste of the `SelfAttentionDecoderLayer` class in `hw4lib/model/decoder_layers.py` with one minor diffrence: it can attend to all positions in the input sequence.
- Implement the `EncoderDecoderTransformer` class in `hw4lib/model/transformers.py`.

### Transformer Sublayers
- Now, Implement the `CrossAttentionLayer` class in `hw4lib/model/sublayers.py`.
- `NOTE`: You should have already implemented the `SelfAttentionLayer`, and `FeedForwardLayer` classes in `hw4lib/model/sublayers.py`.
- Run the cell below to check your implementation.
"""

!python -m tests.test_sublayer_crossattention

"""### Transformer Cross-Attention Decoder Layer
- Implement the `CrossAttentionDecoderLayer` class in `hw4lib/model/decoder_layers.py`.
- Then run the cell below to check your implementation.

"""

!python -m tests.test_decoderlayer_crossattention

"""### Transformer Self-Attention Encoder Layer
- Implement the `SelfAttentionEncoderLayer` class in `hw4lib/model/encoder_layers.py`.
- Then run the cell below to check your implementation.



"""

!python -m tests.test_encoderlayer_selfattention

"""### Encoder-Decoder Transformer

- Implement the  `EncoderDecoderTransformer` class in `hw4lib/model/transformers.py`.
- Then run the cell below to check your implementation.
"""

!python -m tests.test_transformer_encoder_decoder

"""## Decoding Implementation
- We highly recommend you to implement the `generate_beam` method of the `SequenceGenerator` class in `hw4lib/decoding/sequence_generator.py`.
- Then run the cell below to check your implementation.
- `NOTE`: This is an optional but highly recommended task for `HW4P2` to ease the journey to high cutoffs!
"""

!python -m tests.test_decoding --mode beam

"""## Trainer Implementation
You will have to do some minor in-filling for the `ASRTrainer` class in `hw4lib/trainers/asr_trainer.py` before you can use it.
- Fill in the `TODO`s in the `__init__`.
- Fill in the `TODO`s in the `_train_epoch`.
- Fill in the `TODO`s in the `recognize` method.
- Fill in the `TODO`s in the `_validate_epoch`.
- Fill in the `TODO`s in the `train` method.
- Fill in the `TODO`s in the `evaluate` method.

`WARNING`: There are no test's for this. Implement carefully!

# Experiments
From this point onwards you may want to switch to a `GPU` runtime.
- `OBJECTIVE`: Optimize your model for `CER` on the test set.

## Config
- You can use the `config.yaml` file to set your config for your ablation study.

---
### Notes:

- Set `tokenization: token_type:` to specify your desired tokenization strategy
- You will need to set the root path to your `hw4p1_data` folder in `data: root:`. This will depend on your setup. For eg. if you are following out setup instruction:
  - `PSC`: `"/local/hw4_data/hw4p1_data"`
  - `Colab:`: `"/content/hw4_data/hw4p1_data"`
- There's extra configurations in the `optimizer` section which will only be relevant if you decide to use the `create_optimizer` function we've provided in `hw4lib/utils/create_optimizer.py`.
- `BE CAREFUL` while setting numeric values. Eg. `1e-4` will get serialized to a `str` while `1.0e-4` gets serialized to float.
"""

# Load optimized configuration targeting CER < 8%
config = {
    'Name': "baimamboukar_optimized_cer8",

    # Tokenization (5k for optimal performance)
    'tokenization': {
        'token_type': "5k",
        'token_map': {
            'char': 'hw4lib/data/tokenizer_jsons/tokenizer_char.json',
            '1k': 'hw4lib/data/tokenizer_jsons/tokenizer_1000.json',
            '5k': 'hw4lib/data/tokenizer_jsons/tokenizer_5000.json',
            '10k': 'hw4lib/data/tokenizer_jsons/tokenizer_10000.json'
        }
    },

    # Dataset with aggressive SpecAugment for better generalization
    'data': {
        'root': "/content/hw4_data/hw4p2_data",
        'train_partition': "train-clean-100",
        'val_partition': "dev-clean",
        'test_partition': "test-clean",
        'subset': 1.0,
        'batch_size': 24,
        'NUM_WORKERS': 4,
        'norm': 'global_mvn',
        'num_feats': 80,
        # Aggressive SpecAugment for CER < 8%
        'specaug': True,
        'specaug_conf': {
            'apply_freq_mask': True,
            'freq_mask_width_range': 15,
            'num_freq_mask': 3,
            'apply_time_mask': True,
            'time_mask_width_range': 80,
            'num_time_mask': 2
        }
    },

    # Maximum Performance Architecture (28-29M parameters)
    'model': {
        'input_dim': 80,
        'time_reduction': 4,
        'reduction_method': 'both',
        'd_model': 576,
        'num_encoder_layers': 8,
        'num_decoder_layers': 6,
        'num_encoder_heads': 12,
        'num_decoder_heads': 12,
        'd_ff_encoder': 2304,
        'd_ff_decoder': 2304,
        'skip_encoder_pe': False,
        'skip_decoder_pe': False,
        'dropout': 0.1,
        'layer_drop_rate': 0.05,
        'weight_tying': True
    },

    # Training configuration
    'training': {
        'use_wandb': True,
        'wandb_run_id': "none",
        'resume': True,
        'gradient_accumulation_steps': 2,
        'wandb_project': "HW4P2"
    },

    # Enhanced Loss Configuration
    'loss': {
        'label_smoothing': 0.15,
        'ctc_weight': 0.4
    },

    # Advanced Optimizer with Layer-wise Learning Rates
    'optimizer': {
        'name': "adamw",
        'lr': 0.0012,
        'weight_decay': 0.02,
        'adamw': {
            'betas': [0.9, 0.98],
            'eps': 1.0e-6,
            'amsgrad': False
        }
    },

    # Advanced Scheduler
    'scheduler': {
        'name': "cosine_warm",
        'cosine_warm': {
            'T_0': 8,
            'T_mult': 2,
            'eta_min': 0.000005,
            'last_epoch': -1
        },
        'warmup': {
            'enabled': True,
            'type': "exponential",
            'epochs': 4,
            'start_factor': 0.01,
            'end_factor': 1.0
        }
    }
}

# Configuration is now defined above - no need to load from file

"""## Tokenizer"""

Tokenizer = H4Tokenizer(
    token_map  = config['tokenization']['token_map'],
    token_type = config['tokenization']['token_type']
)

"""## Datasets"""

def clean():
  # Clear Python module cache
  import sys

  # Remove cached modules
  modules_to_remove = []
  for module_name in sys.modules.keys():
      if module_name.startswith('hw4lib'):
          modules_to_remove.append(module_name)

  for module_name in modules_to_remove:
      del sys.modules[module_name]

  print(f"Cleared {len(modules_to_remove)} cached modules")

train_dataset = ASRDataset(
    partition=config['data']['train_partition'],
    config=config['data'],
    tokenizer=Tokenizer,
    isTrainPartition=True,
    global_stats=None  # Will compute stats from training data
)

# TODO: Get the computed global stats from training set
global_stats = None
if config['data']['norm'] == 'global_mvn':
    global_stats = (train_dataset.global_mean, train_dataset.global_std)
    print(f"Global stats computed from training set.")

val_dataset = ASRDataset(
    partition=config['data']['val_partition'],
    config=config['data'],
    tokenizer=Tokenizer,
    isTrainPartition=False,
    global_stats=global_stats
)

test_dataset = ASRDataset(
    partition=config['data']['test_partition'],
    config=config['data'],
    tokenizer=Tokenizer,
    isTrainPartition=False,
    global_stats=global_stats
)

gc.collect()

"""## Dataloaders"""

train_loader    = DataLoader(
    dataset     = train_dataset,
    batch_size  = config['data']['batch_size'],
    shuffle     = True,
    num_workers = config['data']['NUM_WORKERS'] if device == 'cuda' else 0,
    pin_memory  = True,
    collate_fn  = train_dataset.collate_fn
)

val_loader      = DataLoader(
    dataset     = val_dataset,
    batch_size  = config['data']['batch_size'],
    shuffle     = False,
    num_workers = config['data']['NUM_WORKERS'] if device == 'cuda' else 0,
    pin_memory  = True,
    collate_fn  = val_dataset.collate_fn
)

test_loader     = DataLoader(
    dataset     = test_dataset,
    batch_size  = config['data']['batch_size'],
    shuffle     = False,
    num_workers = config['data']['NUM_WORKERS'] if device == 'cuda' else 0,
    pin_memory  = True,
    collate_fn  = test_dataset.collate_fn
)

gc.collect()

"""### Dataloader Verification"""

verify_dataloader(train_loader)

verify_dataloader(val_loader)

verify_dataloader(test_loader)

"""## Calculate Max Lengths
Calculating the maximum transcript length across your dataset is a crucial step when working with certain transformer models.
-  We'll use sinusoidal positional encodings that must be precomputed up to a fixed maximum length.
- This maximum length is a hyperparameter that determines:
  - How long of a sequence your model can process
  - The size of your positional encoding matrix
  - Memory requirements during training and inference
- `Requirements`: For this assignment, ensure your positional encodings can accommodate at least the longest sequence in your dataset to prevent truncation. However, you can set this value higher if you anticipate using your languagemodel to work with longer sequences in future tasks (hint: this might be useful for P2! üòâ).
- `NOTE`: We'll be using the same positional encoding matrix for all sequences in your dataset. Take this into account when setting your maximum length.
"""

max_feat_len       = max(train_dataset.feat_max_len, val_dataset.feat_max_len, test_dataset.feat_max_len)
max_transcript_len = max(train_dataset.text_max_len, val_dataset.text_max_len, test_dataset.text_max_len)
max_len            = max(max_feat_len, max_transcript_len)

print("="*50)
print(f"{'Max Feature Length':<30} : {max_feat_len}")
print(f"{'Max Transcript Length':<30} : {max_transcript_len}")
print(f"{'Overall Max Length':<30} : {max_len}")
print("="*50)

"""## Wandb"""

wandb.login(key="57041ac4a63a2bba0cc297951f9d48fd6a98fc6d")
wandb.init(project="HW4P2", entity="idlf25")

"""## Training

Every time you run the trainer, it will create a new directory in the `expts` folder with the following structure:
```
expts/
    ‚îî‚îÄ‚îÄ {run_name}/
        ‚îú‚îÄ‚îÄ config.yaml
        ‚îú‚îÄ‚îÄ model_arch.txt
        ‚îú‚îÄ‚îÄ checkpoints/
        ‚îÇ   ‚îú‚îÄ‚îÄ checkpoint-best-metric-model.pth
        ‚îÇ   ‚îî‚îÄ‚îÄ checkpoint-last-epoch-model.pth
        ‚îú‚îÄ‚îÄ attn/
        ‚îÇ   ‚îî‚îÄ‚îÄ {attention visualizations}
        ‚îî‚îÄ‚îÄ text/
            ‚îî‚îÄ‚îÄ {generated text outputs}
```

### Training Strategy 1: Cold-Start Trainer

#### Model Load (Default)
"""

model_config = config['model'].copy()
model_config.update({
    'max_len': max_len,
    'num_classes': Tokenizer.vocab_size
})

model = EncoderDecoderTransformer(**model_config)

# Get some inputs from the train dataloader
for batch in train_loader:
    padded_feats, padded_shifted, padded_golden, feat_lengths, transcript_lengths = batch
    break

total_param = sum(p.numel() for p in model.parameters() if p.requires_grad)
assert total_param < 30_000_000, f"Total trainable parameters ({total_param}) exceeds 30 million."

model_stats = summary(model, input_data=[padded_feats, padded_shifted, feat_lengths, transcript_lengths])
print(model_stats)

"""#### Initialize Trainer

If you need to reload the model from a checkpoint, you can do so by calling the `load_checkpoint` method.

```python
checkpoint_path = "path/to/checkpoint.pth"
trainer.load_checkpoint(checkpoint_path)
```

"""

clean()

trainer = ASRTrainer(
    model=model,
    tokenizer=Tokenizer,
    config=config,
    run_name="run.1",
    config_file="config.yaml",
    device=device
)

"""### Setup Optimizer and Scheduler

You can set your own optimizer and scheduler by setting the class members in the `LMTrainer` class.
Eg:
```python
trainer.optimizer = optim.AdamW(model.parameters(), lr=config['optimizer']['lr'], weight_decay=config['optimizer']['weight_decay'])
trainer.scheduler = optim.lr_scheduler.CosineAnnealingLR(trainer.optimizer, T_max=config['training']['epochs'])
```

We also provide a utility function to create your own optimizer and scheduler with the congig and some extra bells and whistles. You are free to use it or not. Do read their code and documentation to understand how it works (`hw4lib/utils/*`).

#### Setting up the optimizer
"""

trainer.optimizer = create_optimizer(
    model=model,
    opt_config=config['optimizer']
)

"""#### Creating a test scheduler and plotting the learning rate schedule"""

test_scheduler = create_scheduler(
    optimizer=trainer.optimizer,
    scheduler_config=config['scheduler'],
    train_loader=train_loader,
    gradient_accumulation_steps=config['training']['gradient_accumulation_steps']
)

plot_lr_schedule(
    scheduler=test_scheduler,
    num_epochs=20,
    train_loader=train_loader,
    gradient_accumulation_steps=config['training']['gradient_accumulation_steps']
)

"""#### Setting up the scheduler"""

trainer.scheduler = create_scheduler(
    optimizer=trainer.optimizer,
    scheduler_config=config['scheduler'],
    train_loader=train_loader,
    gradient_accumulation_steps=config['training']['gradient_accumulation_steps']
)

"""#### Train
- Set your epochs and start training!
- `NOTE`: A `scheduler` gets initialized in this call based on the config.
"""

# Train for more epochs to achieve CER < 8%
trainer.train(train_loader, val_loader, epochs=25)

"""#### Comprehensive Beam Search Evaluation for CER < 8%

We'll evaluate with multiple beam search configurations to find the optimal CER performance.
"""

def evaluate_with_beam_search_configs(trainer, test_loader, max_transcript_len):
    """Evaluate model with multiple beam search configurations"""

    # Define multiple beam search configurations for optimal CER
    beam_configs = {
        'greedy': {
            'num_batches': None,
            'beam_width': 1,
            'temperature': 1.0,
            'repeat_penalty': 1.0
        },
        'beam_5': {
            'num_batches': None,
            'beam_width': 5,
            'temperature': 1.0,
            'repeat_penalty': 1.1
        },
        'beam_10': {
            'num_batches': None,
            'beam_width': 10,
            'temperature': 0.9,
            'repeat_penalty': 1.15
        },
        'beam_15': {
            'num_batches': None,
            'beam_width': 15,
            'temperature': 0.8,
            'repeat_penalty': 1.2
        },
        'beam_20_aggressive': {
            'num_batches': None,
            'beam_width': 20,
            'temperature': 0.7,
            'repeat_penalty': 1.25
        }
    }

    best_config = None
    best_cer = float('inf')
    all_results = {}

    print("=" * 80)
    print("COMPREHENSIVE BEAM SEARCH EVALUATION FOR CER < 8%")
    print("=" * 80)

    for config_name, config in beam_configs.items():
        print(f"\nEvaluating with {config_name}...")
        print(f"  Beam Width: {config['beam_width']}")
        print(f"  Temperature: {config['temperature']}")
        print(f"  Repeat Penalty: {config['repeat_penalty']}")

        # Run recognition
        results = trainer.recognize(
            test_loader,
            config,
            config_name=config_name,
            max_length=max_transcript_len
        )

        # Store results
        all_results[config_name] = results

        # If validation set with references, calculate CER
        if results and 'reference' in results[0]:
            references = [r['reference'] for r in results]
            hypotheses = [r['hypothesis'] for r in results]

            # Calculate CER using trainer's metric function
            metrics = trainer._calculate_asr_metrics(references, hypotheses)
            cer = metrics['cer']

            print(f"  CER: {cer:.2f}%")
            print(f"  WER: {metrics['wer']:.2f}%")

            # Track best configuration
            if cer < best_cer:
                best_cer = cer
                best_config = config_name

            # Log to wandb
            wandb.log({f"{config_name}_cer": cer, f"{config_name}_wer": metrics['wer']})
        else:
            print("  Test set - no CER calculation available")

    if best_config:
        print(f"\nBEST CONFIGURATION: {best_config}")
        print(f"BEST CER: {best_cer:.2f}%")

        if best_cer < 8.0:
            print("SUCCESS: Achieved CER < 8%!")
        else:
            print(f"Need improvement: Current best CER ({best_cer:.2f}%) > 8%")

        wandb.log({"best_cer": best_cer, "best_config": best_config})

    return all_results, best_config

# Evaluate on validation set first to find best configuration
print("\nEVALUATING ON VALIDATION SET:")
val_results, best_config_val = evaluate_with_beam_search_configs(trainer, val_loader, max_transcript_len)

# Evaluate on test set with all configurations
print("\nEVALUATING ON TEST SET:")
test_results, _ = evaluate_with_beam_search_configs(trainer, test_loader, max_transcript_len)

# Use the best configuration from validation set for final submission
if best_config_val and best_config_val in test_results:
    final_config = best_config_val
    print(f"\nUsing best validation configuration for submission: {final_config}")
else:
    final_config = 'beam_10'  # Default fallback
    print(f"\nUsing default configuration for submission: {final_config}")

# Extract final predictions
final_results = test_results[final_config]
if 'generated' in final_results[0]:
    generated = [r['generated'] for r in final_results]
else:
    generated = [r.get('hypothesis', r.get('generated', '')) for r in final_results]

results_df = pd.DataFrame(
    {
        'id': range(len(generated)),
        'transcription': generated
    }
)

print(f"\nFinal submission uses {final_config} configuration")
print(f"Generated {len(generated)} transcriptions")

# Cleanup (Will end wandb run)
trainer.cleanup()

"""## Submit to Kaggle

### Authenticate Kaggle
In order to use the Kaggle‚Äôs public API, you must first authenticate using an API token. Go to the 'Account' tab of your user profile and select 'Create New Token'. This will trigger the download of kaggle.json, a file containing your API credentials.
- `TODO`: Set your kaggle username and api key here based on the API credentials listed in the kaggle.json
"""

import os
os.environ["KAGGLE_USERNAME"] = "bbaimamb"
os.environ["KAGGLE_KEY"] = "8fcf5086644f33fb2aa3f4a83d38ceb6"

results_df.head()

"""### Submit"""

results_df.to_csv("results.csv", index=False)
!kaggle competitions submit -c 11-785-hw-4-p-2-automatic-speech-recognition-f-25 -f results.csv -m "beam_search_optimized"

"""#### TODO: Generate a model_metadata.json file to save your model's data (due 48 hours after Kaggle submission deadline OR the day of slack submission)"""

import json, os, sys, torch, datetime
################################
# TODO: Keep the model_metadata.json
# file safe for submission ater.
################################
def is_colab():
    return "google.colab" in sys.modules and "COLAB_GPU" in os.environ

def is_kaggle():
    return "KAGGLE_KERNEL_RUN_TYPE" in os.environ or "KAGGLE_URL_BASE" in os.environ

def generate_model_submission_file(model):
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M")
    json_filename = f"model_metadata_{timestamp}.json"

    # Create JSON with parameter count, model architecture, and predictions
    output_json = {
        "parameter_count": sum(p.numel() for p in model.parameters() if p.requires_grad),
        "model_architecture": str(model),
    }

    # Save metadata JSON
    with open(json_filename, "w") as f:
        json.dump(output_json, f, indent=2)

    # Download / display link depending on environment
    if is_colab():
        from google.colab import files
        print(f"OK: Saved as {json_filename}. Downloading in Colab...")
        files.download(json_filename)

    elif is_kaggle():
        from IPython.display import FileLink, display
        print("#" * 100)
        print(f"OK: Your submission file `{json_filename}` has been generated.")
        print("TODO: Click the link below.")
        print("1. The file will open in a new tab.")
        print("2. Right-click anywhere in the new tab and select 'Save As...'")
        print("3. Save the file to your computer with the `.json` extension.")
        print("You MUST submit this file to Autolab if this is your best submission.")
        print("#" * 100 + "\n")
        display(FileLink(json_filename))

    else:
        print(f"OK: saved model data saved to: '{json_filename}'")
        print("REQUIRED to submit to Autolab if these are the best model weights.")

generate_model_submission_file(model)
#### IMPORTANT: Do NOT change the name of the model_metadata_....json file!!

"""## TODO: fill in your submission requirements

### Notes:

- You will need to set the root path to your submission files (eg. MODEL_METADATA_JSON, NOTEBOOK_PATH, HW4LIB_PATH). This will depend on your setup. For eg. if you are following our setup instruction:
  - `Colab:`: `"/content/..."`In the left file pane, right-click the desired file or folder and select ‚ÄúCopy path‚Äù.
  - `PSC`: `"/jet/home/<your_username>/..."` You can check the files in this path by running: ```!ls /jet/home/<your_username>/```

Kindly modify your configurations to suit your ablations and be keen to include your name.
"""

####################################
#             README
####################################

# TODO: Please complete all components of this README
README = """
- **Model**: Model archtiecture description. Anything unique? Any specific architecture shapes or strategies?
- **Training Strategy**: optimizer + scheduler + loss function + any other unique ideas
- **Augmentations**: augmentations if used. If augmentations weren't used, then ignore
- **Notebook Execution**: Any instructions required to run your notebook.
"""

####################################
#       Credentials (Optional)
####################################

# These are not required **IF** you have run the cells to declare these variables above.
# If you would like to paste your credentials here again, feel free to:
# OPTIONAL: Fill these out if you do not want to re-run previous cells to re-initialize these credential variables

KAGGLE_USERNAME = "bbaimamb" #TODO
KAGGLE_API_KEY = "8fcf5086644f33fb2aa3f4a83d38ceb6" #TODO
WANDB_API_KEY = "57041ac4a63a2bba0cc297951f9d48fd6a98fc6d" #TODO


####################################
#             Wandb Logs
####################################

# TODO: Your wandb project url should look like https://wandb.ai/username-or-team-name/project-name
#(Take these parameters and put them in the variables below)

WANDB_USERNAME_OR_TEAMNAME = "idlf25" # TODO: Put your username-or-team-name here
WANDB_PROJECT = "HW4P2" # TODO: Put your project-name

####################################
#         Notebook & Files
####################################

# TODO: Download HW4P2 Notebook (if on colab or kaggle) and upload both your HW4P2 notebook + model_metadata_*.json to your file system.
# TODO: For each file, obtain the file paths and put them below.

# TODO: COLAB INSTRUCTIONS:
# * With Colab, upload your desired file (notebook or model_metadata.json) to "Files"
# * Right-click the file, click "Copy Path,"
# * Paste the path below.

# TODO: KAGGLE INSTRUCTIONS:
# * First download a copy of your notebook with "File > Download Notebook"
# Then...
# * Click "File" in the top left of the screen
# * Go to "Upload Input > Upload Model"
# * Upload your notebook file.
# * For "Model Name" put HW4P2_Final_Submission
# * For "Framework" put "Other"
# * For "License" put "Other"
# * Click "Upload another file" and upload your model_metadata####.json file as well.
# * Now, on your right in your "Models" section, you should see a new folder with your submission files.
# * Click on the "Copy File Path" buttons for the notebook and json file and paste them below.

# TODO: Linux system:
# * Simply upload or find the path of your notebook file and model_metadata###.json file, and paste them here.

NOTEBOOK_PATH = "/content/drive/MyDrive/hw4p2/HW4P2_Student_Starter_Notebook.ipynb" # TODO: Put your HW4P2 notebook path here
MODEL_METADATA_JSON = "/content/model_metadata_2025-07-14_21-42.json" # TODO: Put your Model Metadata path json file here (see end of HW4P2 Code Notebook to get this file)
HW4LIB_PATH = "/content/hw4lib" # TODO: Put your hw4lib path here

####################################
#         Additional Files
####################################

ADDITIONAL_FILES = [ # TODO: Upload any files and add any paths to any additional files you would like to include in your submission, otherwise, leave this empty
]

####################################
#         SLACK SUBMISSION
####################################

ENABLE_SLACK_SUBMISSION = False # TODO: Set this to true if you are submitting to the Slack competition

####################################
#     Creating the Submission
####################################

# TODO: Once the README, wandb information, and file paths are filled in, run this cell,
# run the "Assignment Backend Functions" in the next cells, and generate the final zip file at the end.

SAFE_SUBMISSION = True # TODO: Set this to False if you want to generate a submission.zip even if you are missing files, otherwise it's recommended to keep this as True

"""# Assignment Backend Submission Functions (DO NOT MODIFY, just run these cells)"""

from datetime import datetime

######################################
#       Assignment Configs
######################################

WANDB_METRIC = "CER"
WANDB_DIRECTION = "descending"
WANDB_TOP_N = 10
WANDB_OUTPUT_PKL = "wandb_top_runs.pkl"

# Kaggle configuration
COMPETITION_NAME = "11-785-hw-4-p-2-automatic-speech-recognition-f-25"
SLACK_COMPETITION_NAME = "slack-hw-4-p-2-f-25"
FINAL_SUBMISSION_DATETIME = datetime.strptime("2025-12-06 00:00:00", "%Y-%m-%d %H:%M:%S")
SLACK_SUBMISSION_DATETIME = datetime.strptime("2025-12-11 00:00:00", "%Y-%m-%d %H:%M:%S")
GRADING_DIRECTION = "descending"
KAGGLE_OUTPUT_JSON = "kaggle_data.json"

SUBMISSION_OUTPUT = "HW4P2_final_submission.zip"

from datetime import datetime, timezone
import zoneinfo

eastern = zoneinfo.ZoneInfo("America/New_York")
FINAL_DEADLINE_UTC = (
    FINAL_SUBMISSION_DATETIME
    .replace(tzinfo=eastern)
    .astimezone(timezone.utc)
)

SLACK_DEADLINE_UTC = (
    SLACK_SUBMISSION_DATETIME
    .replace(tzinfo=eastern)
    .astimezone(timezone.utc)
)

ACKNOWLEDGEMENT_MESSAGE = """
Submission of this file and assignment indicate the student's agreement to the following Aknowledgement requirements:
Setting the ACNKOWLEDGED flag to True indicates full understanding and acceptance of the following:
1. Slack days may ONLY be used on P2 FINAL (not checkpoint) submission. I.e. you may use slack days to submit final P2 kaggle scores (such as this one) later on the **SLACK KAGGLE COMPETITION** at the expense of your Slack days.
2. The final autolab **code submission is due 48 hours after** the conclusion of the Kaggle Deadline (or, the same day as your final kaggle submission).
3. Course staff will require your kaggle username here, and then will pull your official PRIVATE kaggle leaderboard score. This submission may result in slight variance in scores/code, but we will check for acceptable discrepancies. Any discrepancies related to modifying the submission code (at the bottom of the notebook) will result in an AIV.
4. You are NOT allowed to use any code that will pre-load models (such as those from Hugging Face, etc.).
   You MAY use models described by papers or articles, but you MUST implement them yourself through fundamental PyTorch operations (i.e. Linear, Conv2d, etc.).
5. You are NOT allowed to use any external data/datasets at ANY point of this assignment.
6. You may work with teammates to run ablations/experiments, BUT you must submit your OWN code and your OWN results.
7. Failure to comply with the prior rules will be considered an Academic Integrity Violation (AIV).
8. Late submissions MUST be submitted through the Slack Kaggle (see writeup for details). Any submissions made to the regular Kaggle after the original deadline will NOT be considered, no matter how many slack days remain for the student.
"""
def save_acknowledgment_file():
    if ACKNOWLEDGED:
        with open("acknowledgement.txt", "w") as f:
            f.write(ACKNOWLEDGEMENT_MESSAGE.strip())
        print("Saved acknowledgement.txt")
        return True
    else:
        print("ERROR: Must set ACKNOWLEDGED = True.")
        return False
# Saves README
def save_readme(readme):
    try:
        with open("README.txt", "w") as f:
            f.write(readme.strip())

        print("Saved README.txt")
    except Exception as e:
        print(f"ERROR: Error occured while saving README.txt: {e}")
        return False

    return True

# Saves wandb logs
import wandb, json, pickle

def save_top_wandb_runs():
    wandb.login(key=WANDB_API_KEY)
    if not ACKNOWLEDGED:
        print("ERROR: Must set ACKNOWLEDGED = True.")
        return False

    api = wandb.Api()
    runs = api.runs(
        f"{WANDB_USERNAME_OR_TEAMNAME}/{WANDB_PROJECT}",
        order=f"{'-' if WANDB_DIRECTION == 'descending' else ''}summary_metrics.{WANDB_METRIC}"
    )
    selected_runs = runs[:min(WANDB_TOP_N, len(runs))]

    if not selected_runs:
        print(f"ERROR: No runs found for {WANDB_USERNAME_OR_TEAMNAME}/{WANDB_PROJECT}. Please check that your wandb credentials (Wandb Username/Team Name, API Key, and Project Name) are correct.")
        return False

    all_data = []
    for run in selected_runs:
        run_data = {
            "id": run.id,
            "name": run.name,
            "tags": run.tags,
            "state": run.state,
            "created_at": str(run.created_at),
            "config": run.config,
            "summary": dict(run.summary),
        }
        try:
            run_data["history"] = run.history(samples=1000)
        except Exception as e:
            run_data["history"] = f"Failed to fetch history: {str(e)}"
        all_data.append(run_data)
    with open(WANDB_OUTPUT_PKL, "wb") as f:
        pickle.dump(all_data, f)

    print(f"OK: Exported {len(all_data)} WandB runs to {WANDB_OUTPUT_PKL}")

    return True
# Saves kaggle information

# Install dependencies silently (only if running on Colab)
import sys

from datetime import datetime
import os, json, requests
def kaggle_login(username, key):
    os.makedirs(os.path.expanduser("~/.kaggle"), exist_ok=True)
    with open(os.path.expanduser("~/.kaggle/kaggle.json"), "w") as f:
        json.dump({"username": username, "key": key}, f)
    os.chmod(os.path.expanduser("~/.kaggle/kaggle.json"), 0o600)


def get_active_submission_config():
    if ENABLE_SLACK_SUBMISSION:
        return SLACK_COMPETITION_NAME, SLACK_DEADLINE_UTC
    return COMPETITION_NAME, FINAL_DEADLINE_UTC

def kaggle_user_exists(usernagbme):
    try:
        return requests.get(f"https://www.kaggle.com/{KAGGLE_USERNAME}").status_code == 200
    except Exception as e:
        print(f"ERROR: Error occured while checking Kaggle user: {e}")
        return False

DEFAULT_SCORE=0
if GRADING_DIRECTION == "ascending":
    DEFAULT_SCORE=0
else:
    DEFAULT_SCORE=1.0

def get_best_kaggle_score(subs):
    def extract_score(s): return float(s.private_score or s.public_score or DEFAULT_SCORE)
    if not subs:
        return None, None
    best = max(subs, key=lambda s: extract_score(s) if GRADING_DIRECTION == "ascending" else -extract_score(s))

    score_type = "private" if best.private_score not in [None, ""] else "public"
    return extract_score(best), score_type

def save_kaggle_json(kaggle_username, kaggle_key):

    kaggle_login(kaggle_username, kaggle_key)

    from kaggle.api.kaggle_api_extended import KaggleApi

    if not ACKNOWLEDGED:
        print("ERROR: Must set ACKNOWLEDGED = True.")
        return False

    if not kaggle_user_exists(KAGGLE_USERNAME):
        print(f"ERROR: User '{KAGGLE_USERNAME}' not found.")
        return False

    comp_name, deadline = get_active_submission_config()

    api = KaggleApi()
    api.authenticate()

    # Get competition submissions
    submissions = [s for s in api.competition_submissions(comp_name) if getattr(s, "_submitted_by", None) == KAGGLE_USERNAME]
    if not submissions:
        print(f"ERROR: No valid submissions found for user [{KAGGLE_USERNAME}] for this competition [{comp_name}]. Slack flag set to [{ENABLE_SLACK_SUBMISSION}]")
        print("Please double check your Kaggle username and ensure you've submitted at least once.")
        return False

    score, score_type = get_best_kaggle_score(submissions)
    result = {
        "kaggle_username": KAGGLE_USERNAME,
        "acknowledgement": ACKNOWLEDGED,
        "submitted_slack": ENABLE_SLACK_SUBMISSION,
        "competition_name": comp_name,
        "deadline": deadline.strftime("%Y-%m-%d %H:%M:%S"),
        "raw_score": score * 100.0,
        "score_type": score_type,
    }

    print(f"OK: Projected score (excluding bonuses) saved as {KAGGLE_OUTPUT_JSON}")
    if score:
        print(f"Best score {score}.")
        with open(KAGGLE_OUTPUT_JSON, "w") as f:
            json.dump(result, f, indent=2)
        return True
    return False

import os
import sys
import zipfile


def create_submission_zip(additional_files, safe_flag):
    if not "ACKNOWLEDGED" in globals() or not ACKNOWLEDGED:
        print("ERROR: Make sure to RUN the Acknowledgement cell (at the top of the notebook). Also, must set ACKNOWLEDGED = True.")
        return

    if (not save_acknowledgment_file()):
        print("ERROR: Make sure to RUN the Acknowledgement cell (at the top of the notebook). Also, must set ACKNOWLEDGED = True.")
        return


    if not "ENABLE_SLACK_SUBMISSION" in globals() or ENABLE_SLACK_SUBMISSION is None:
        print("ERROR: \"ENABLE_SLACK_SUBMISSION\" variable is not defined. \nTODO: Make sure to RUN the cell (A few cells up at the beginning of the submission section). \nMake sure to set the ENABLE_SLACK_SUBMISSION checkbox if you're on colab, or set the parameter correctly set on other platforms \n(if you are submitting through the SLACK submission).")
        return

    if not "README" in globals() or not README:
        print("ERROR: Make sure to RUN the README cell(above your credentials cell).")
        return

    if (not save_readme(README)):
        print("ERROR: Error while saving the README file. Make sure to complete and RUN the README cell(above your credentials cell).")
        return

    if (not save_top_wandb_runs()):
        return

    if not "KAGGLE_USERNAME" in globals() or not "KAGGLE_API_KEY" in globals() or not KAGGLE_USERNAME or not KAGGLE_API_KEY:
        print("ERROR: Make sure to set KAGGLE_USERNAME and KAGGLE_API_KEY for this code submission.")
        return

    if (not save_kaggle_json(KAGGLE_USERNAME, KAGGLE_API_KEY)):
        print(f"ERROR: An error occured while retrieve kaggle information from username [{KAGGLE_USERNAME}] from competition [{get_active_submission_config()[0]}] with slack flag set to [{ENABLE_SLACK_SUBMISSION}]. Please check your kaggle username, key, and submission.")
        return

    files_to_zip = [
        "acknowledgement.txt",
        "README.txt",
        KAGGLE_OUTPUT_JSON,
        WANDB_OUTPUT_PKL,
        MODEL_METADATA_JSON,
        NOTEBOOK_PATH,
        HW4LIB_PATH,
    ] + additional_files

    missing_files = False

    with zipfile.ZipFile(SUBMISSION_OUTPUT, "w") as zipf:
        for file_path in files_to_zip:
            if os.path.exists(file_path):
                arcname = os.path.basename(file_path)  # flatten path
                zipf.write(file_path, arcname=arcname)
                print(f"OK: Added {arcname}")
            else:
                missing_files = True
                print(f"ERROR: Missing file: {file_path}")

    if missing_files:
        if safe_flag:
            raise "ERROR: Missing files with safety flag set to True. Please upload any necessary files, ensure you have the correct paths and rerun all cells."
        else:
            print("WARNING: Missing files with safety flag set to False. Submission may be incomplete.")

    if "google.colab" in sys.modules:
        from google.colab import files
        files.download(SUBMISSION_OUTPUT)

    print("Final submission saved as:", SUBMISSION_OUTPUT)

"""# File Generation (TODO: Check file generation outputs for any errors)

### For Colab and PSC users:
"""

create_submission_zip(ADDITIONAL_FILES, SAFE_SUBMISSION)

#TODO: If the HW4P2_final_submission.zip file does not
# automatically bring up a donwload pop-up
# Then make sure to entire the files and
#manually download the checkpoint_submission.json file.