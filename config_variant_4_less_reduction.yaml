Name: "variant_4_less_reduction_15m"

###### Tokenization --------------------------------
tokenization:
  token_type: "5k"
  token_map:
      'char': 'hw4lib/data/tokenizer_jsons/tokenizer_char.json'
      '1k'  : 'hw4lib/data/tokenizer_jsons/tokenizer_1000.json'
      '5k'  : 'hw4lib/data/tokenizer_jsons/tokenizer_5000.json'
      '10k' : 'hw4lib/data/tokenizer_jsons/tokenizer_10000.json'

###### Dataset Configuration -----------------------------------------------------
data:
  root                 : "/content/hw4_data/hw4p2_data"
  train_partition      : "train-clean-100"
  val_partition        : "dev-clean"
  test_partition       : "test-clean"
  subset               : 1.0
  batch_size           : 24
  NUM_WORKERS          : 4
  norm                 : 'global_mvn'
  num_feats            : 80

  # Balanced SpecAugment
  specaug                   : True
  specaug_conf:
    apply_freq_mask         : True
    freq_mask_width_range   : 10
    num_freq_mask           : 2
    apply_time_mask         : True
    time_mask_width_range   : 50
    num_time_mask           : 2

###### Model: Less Time Reduction 15M parameters ------------
model:
  input_dim: 80
  time_reduction: 4  # Less aggressive downsampling
  reduction_method: 'conv'  # Different reduction method

  # Adjusted for less reduction
  d_model: 240
  num_encoder_layers: 4
  num_decoder_layers: 4
  num_encoder_heads: 8
  num_decoder_heads: 8
  d_ff_encoder: 960
  d_ff_decoder: 960
  skip_encoder_pe: False
  skip_decoder_pe: False

  dropout: 0.1
  layer_drop_rate: 0.06
  weight_tying: True

###### Training Configuration ---------------------------------------------------
training:
  use_wandb                   : True
  wandb_run_id                : "none"
  resume                      : True
  gradient_accumulation_steps : 2  # Needed for smaller batch
  wandb_project               : "HW4P2"

###### Loss Configuration -------------------------------------------------------
loss:
  label_smoothing: 0.13
  ctc_weight: 0.4

###### Optimizer Configuration --------------------------------------------------
optimizer:
  name: "adamw"
  lr: 0.0015
  weight_decay: 0.012
  adamw:
    betas: [0.9, 0.98]
    eps: 1.0e-6
    amsgrad: False

###### Scheduler ---------------------------------------
scheduler:
  name: "cosine"
  cosine:
    T_max: 22
    eta_min: 0.00001
    last_epoch: -1
  warmup:
    enabled: True
    type: "exponential"
    epochs: 3
    start_factor: 0.1
    end_factor: 1.0

###### Beam Search Configuration --------------------
beam_search:
  configs:
    beam_5:
      beam_width: 5
      temperature: 1.0
      repeat_penalty: 1.1
      length_penalty: 0.8
    beam_10:
      beam_width: 10
      temperature: 0.9
      repeat_penalty: 1.15
      length_penalty: 0.9
    beam_15:
      beam_width: 15
      temperature: 0.85
      repeat_penalty: 1.2
      length_penalty: 1.0